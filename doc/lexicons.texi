@chapter Lexicons

This chapter covers method for finding the pronunciation of
a word.  This is either by a lexicon (a large list of words
and their pronunciations) or by some method of letter to
sound rules.

@section Word pronunciations

A pronunciation in Festival requires not just a list of
phones but also a syllabic structure.  In some language sthe
syllabic structure is very simple and well defined and
can be unabiguously derived from a phone string.  In English
however this may not always be the case (compound nouns
being the difficult case).  

The lexicon structure that is basically available in Festival takes both
a word and a part of speech (and arbitrary token) to find the given
pronunciation.  For English this is probabaly the optimal form, although
there exist homogrpahs in the language, the word itself and a fairly
broad part of speech tag will mostly identify the proper pronunciation.

An example entry is 
@lisp
("photography"
 n
 (((f @ ) 0) ((t o g) 1) ((r @ f) 0) ((ii) 0)))
@end lisp
Not that in addition to explicit marking of sylables a stress 
value is also given (0 or 1).  In some languages lexical is
fully predictable, in others highly irregular.  In some this
field may be more approriately used for an other purpose,
e.g. tone type in Chinese. 

There may be other languages which require a more complex (less
complex) format and the decision to use some other format rather
than this one is up to you.

@cindex morphology
Currently there is only residual support fo morphological analysis in
Festival.  A finite state tranducer based analyser for English based on
the work in @cite{ritchie92} is included in
@file{festival/lib/engmorph.scm} and
@file{festival/lib/engmorphsyn.scm}.  But this should be considered
experimental at best.  Give the lack of such an analyser our lexicons
need to list not only based forms of words but also all their
morphological variants.  This is (more or less) acceptable in languages
such as English or French but which languages with richer morphology
such as German it may seem an unnecessary requirement.  Agglutenative
languages such as Finnish and Turkish this appears to be even more a
restriction.  This is probably true but this current restriction
not necessary hopeless.  We have successfully build very good
letter to sound rules for German, a language with a rich morphology
which allows the system to properly predict pronounciations of
morhological variants of root words it has not seen before.
We have not yet done any experiments with Finnish or Turkish
but see this technique would work, (though of course developing
a properly morphological analyser would be better).

@section Lexicons and addenda

The basic assumption in Festival is that you will have a large lexicon,
tens of thousands of entries, that is a used as a standard part of an
implementation of a voice.  Letter to sound rules are used as back up
when a word is not explicitly listed.  This view is based on how English
is best dealt with.  However this is a very flexible view, An explicit
lexicon isn't necessary in Festival and it may be possibel to do much of
the work in letter to sound rules.  This is how we have implemented
Spanish.  However even when there is strong relationship between the
letters in a word and their pronunciation we still find the a lexicon
useful.  For Spanish we still use the lexicon for symbols such as
@samp{$}, @samp{%}, individual letters, as well as irregular
pronunciations.

In addition to a large lexicon Festival also supports a smaller list
called an @emph{addenda} this is primarily provided to allow specific
applications and users to add entries that aren't in the existing
lexicon.

@section Out of vocabulary words

Because its impossible to list all words in a natural language for
general text to speech you will need to provide something to pronounce
out of vocabulary words.  In some language sthis is easy but in other's
it is very hard.  No matter what you do you @emph{must} provide
something even if it is simply replacing the unknown word with the word
@samp{unknown} (or its local language equivalent).  By default a lexicon
in Festival will throw an error if a requested word isn't found.  To
change this you can set the @code{lts_method}.  Most usefully you can
reset this to the name of function, which takes a word and a part of
speech specification and returns a word pronuciation as described above.

FOr example is we are always going to return the 
word @code{unknown} but print a warning the the word is being
ignored a suitable function is
@lisp
(define (mylex_lts_function word feats)
"Deal with out of vocabulary word."
  (format t "unknown word: %s\n" word)
  '("unknown" n (((uh n) 1) ((n ou n) 1))))
@end lisp
Note the pronunciation of @samp{unknown} must be in the appropriate
phone set.  Also the syllabic structure is required.  You need to
specify this function for your lexicon as follows
@lisp
(lex.set.lts.method 'mylex_lts_function)
@end lisp

At one level above merely identifying out of vocabulary words, they can
be spelled, this of course isn't ideal but it will allow the basic
information to be passed over to the listener.  This can be done
with the out of vocabulary function, as follows.
@lisp
(define (mylex_lts_function word feats)
"Deal with out of vocabulary wordm by spelling out the letters in the
word."
 (if (equal? 1 (length word))
     (begin
       (format t "the character %s is missing from the lexicon\" word)
       '("unknown" n (((uh n) 1) ((n ou n) 1))))
     (cons
      word
      'n
      (apply
       append
       (mapcar
        (lambda (letter)
         (car (cdr (cdr (lex.lookup letter 'n)))))
        (symbolexplode word))))))
@end lisp
A few point are worth noting in this function.  This recusively calls
the lexical lookup function on the characters in a word.  Each letter
should appear in the lexicon with its pronuncitation (in isolation).
But a check is made to ensure we don't recurse for ever.  The
@code{symbolexplode} function assumes that that letters are single
bytes, which may not be true for some languages and that function would
need to be replaced for that language.  Note that we append the
syllables of each of the letters int he word.  For long words this might
be too naive as there could be internal prosodic structure in such a
spelling that this method would not allow for.  In that case you would
want letters to be words thus the symbol expolsion to happen at the
token to word level.  Also the above function assumes that the part of
speech for letters is @code{n}.  This is only really important where
letters are homographs in languages so this can be used to distingush
which pronunciation you require (cf. @samp{a} in English or @samp{y} in
French).

@section Letter to sound rules by hand

@cindex letter to sound rules by hand
@cindex LTS
For many languages there is a systematic relationship between the
written form of a word and its pronunciation.  For some language this
can be fairly easy to write down, by hand. In Festival there is a letter
to sound rule system that allows rules to be written.  This rule system,
described in detail in the Festival manual itself is what you should use
if you are going to write rules by hand.  There is also an automatic
training method fully described in the next sections, which produces
CART trees which although are easy to interpret are probably
unsuitable as a notation for hand specification.

When writing a rule system it is often useful to do it in multiple
passes.  The Spanish diphone voice distributed as
@file{festvox_ellpc11k.tar.gz} offers a good example of such 
a use. A set of cascaded LTS rule sets is used to trasnfer
the basic word to a full accented, syllabified string of symbols which
is then converted into the bracketed from used by Festival.  The
levels are normalisations (downcasing and accent normalization), 
convertion to pronunciation, syllabification, stress and finally
identifying weak vowels.  Splitting the conversion tasks like
this can often make writing the rules much easier, though care should
be taken to ensure you don't mix up what you think are letters
and what you think are phones.

The LTS rule system is a little primitive and lacks some syntactic sugar
(sets etc.) that would make writing rules easier.  In their present form
you need to be very explicit.  Testing your rule set can be done in
Festival in isolation (and should be done so, rather than by actual
synthesis).  The funciton @code{lts.apply} allows you to apply a LTS
rule set to a word or list of symbols.  See the manual and the
Spanish example for more details.

@section Building letter to sound rules

For some languages the writing of a rule system is too difficult.
Although there have been many valiant attempts to do so for language
slike English life is basically too short to do this.  Therefore we also
include a method for automatically building LTS rules sets for a lexicon
of pronunciations.  This technique has successfully been used from
English (British and American), French and German.  The difficulty and
appropriateness of using letter to sound rules is very language
dependent,

The following outlines the processes involved in building a letter to
sound model for a language given a large lexicon of pronunciations.
This technique is likely to work for most European languages (including
Russian) but doesn't seem particularly suitable for very language
alphabet languages like Japanese and Chinese.  The process described
here is not (yet) fully automatic but the hand intervention required is
small and may easily be done even by people with only a very little
knowledge of the language being dealt with.

The process involves the following steps
@itemize @bullet
@item 
Pre-processing lexicon into suitable training set
@item
Defining the set of allowable pairing of letters to phones.  (We intend
to do this fully automatically in future versions).
@item
Constructing the probabilities of each letter/phone pair.
@item
Aligning letters to an equal set of phones/_epsilons_.
@item 
Extracting the data by letter suitable for training.
@item
Building CART models for predicting phone from letters (and context).
@item
Building additional lexical stress assignment model (if necessary).
@end itemize
All except the first two stages of this are fully automatic.

Before building a model its wise to think a little about what you want
it to do.  Ideally the model is an auxiluary to the lexicon so only
words not found in the lexicon will require use of the letter to sound
rules.  Thus only unusual forms are likely to require the rules.  More
precisely the most common words, often having the most non-standard
pronunciations, should probably be explicitly listed always.  It is
possible to reduce the size of the lexicon (sometimes drastically) by
removing all entries that the training LTS model correctly predicts.

Before starting it is wise to consider removing some entries from the
lexicon before training, I typically will remove words under 4 letters
and if part of speech information is available I remove all function
words, ideally only training from nouns verbs and adjectives as these
are the most likely forms to be unknown in text.  It is useful to have
morphologically inflected and derived forms in the training set as it is
often such variant forms that not found in the lexicon even though their
root morpheme is.  Note that in many forms of text, proper names are the
most common form of unknown word and even the technique presented here
may not adequately cater for that form of unknown words (especially if
they unknown words are non-native names).  This is all stating that this
may or may not be appropriate for your task but the rules generated by
this learning process have in the examples we've done been much better
than what we could produce by hand writing rules of the form described
in the previous section.

First preprocess the lexicon into a file of lexical entries to be used
for training, removing functions words and changing the head words to
all lower case (may be language dependent).  The entries should be of
the form used for input for Festival's lexicon compilation.  Specifical
the pronunciations should be simple lists of phones (no
syllabification).  Depending on the language, you may wish to remve the
stressing---for examples here we have though later tests suggest that we
should keep it in even for English.  Thus the training set should look
something like
@lisp
("table" nil (t ei b l))
("suspicious" nil (s @@ s p i sh @@ s))
@end lisp
It is best to split the data into a training set and a test set 
if you wish to know how well your training has worked.  In our
tests we remove every tenth entry and put it in a test set.  Note this
will mean our test results are probably better than if we removed
say the last ten in every hundred.

The second stage is to define the set of allowable letter to phone
mappings irrespective of context.  This can sometimes be initially done
by hand then checked against the training set.  Initially constract a
file of the form
@lisp
(require 'lts_build)
(set! allowables 
      '((a _epsilon_)
        (b _epsilon_)
        (c _epsilon_)
        ...
        (y _epsilon_)
        (z _epsilon_)
        (# #)))
@end lisp
All letters that appear in the alphabet should (at least) map to
@code{_epsilon_}, including any accented characters that appear in that
language.  Note the last two hashes.  These are used by to denote
beginning and end of word and are automatically added during training,
they must appear in the list and should only map to themselves.

To incrementally add to this allowable list run festival as
@lisp
festival allowables.scm 
@end lisp
and at the prompt type
@lisp
festival> (cummulate-pairs "oald.train")
@end lisp
with your train file.  This will print out each lexical entry
that couldn't be aligned with the current set of allowables.  At the
start this will be every entry.  Looking at these entries add 
to the allowables to make alignment work.  For example if the
following word fails
@lisp
("abate" nil (ah b ey t)) 
@end lisp
Add @code{ah} to the allowables for letter @code{a}, @code{b} to
@code{b}, @code{ey} to @code{a} and @code{t} to letter @code{t}.  After
doing that restart festival and call @code{cummulate-pairs} again.
Incrementally add to the allowable pairs until the number of failures
becomes accceptable.  Often there are entries for which there is no real
relationship between the letters and the pronunciation such as in
abbreviations and foreign words (e.g. "aaa" as "t r ih p ax l ey").  For
the lexicons I've used the technique on less than 10 per thousand fail
in this way.

It is worth while being consistent on defining your set of allowables.
(At least) two mappings are possible for the letter sequence
@code{ch}---having letter @code{c} go to phone @code{ch} and letter
@code{h} go to @code{_epsilon_} and also letter @code{c} go to phone
@code{_epsilon_} and letter @code{h} goes to @code{ch}.  However only
one should be allowed, we preferred @code{c} to @code{ch}.  

It may also be the case that some letters give rise to more than one
phone.  For example the letter @code{x} in English is often pronunced as
the phone combination @code{k} and @code{s}.  To allow this, use the
multiphone @code{k-s}.  Thus the multiphone @code{k-s} will be predicted
for @code{x} in some context and the model will separate it into two
phones while it also ignoring any predicted @code{_epsilons_}.  Note that
multiphone units are relatively rare but do occur.  In English, letter
@code{x} give rise to a few, @code{k-s} in @code{taxi}, @code{g-s} in
@code{example}, and sometimes @code{g-zh} and @code{k-sh} in
@code{luxury}.  Others are @code{w-ah} in @code{one}, @code{t-s} in
@code{pizza}, @code{y-uw} in @code{new} (British), @code{ah-m} in
@code{-ism} etc.  Three phone multiphone are much rarer but may exist, they
are not supported by this code as is, but such entries should probably
be ignored.  Note the @code{-} sign in the multiphone examples is
significant and is used to indentify multiphones.

The allowables for OALD end up being
@lisp
(set! allowables 
       '
      ((a _epsilon_ ei aa a e@@ @@ oo au o i ou ai uh e)
       (b _epsilon_ b )
       (c _epsilon_ k s ch sh @@-k s t-s)
       (d _epsilon_ d dh t jh)
       (e _epsilon_ @@ ii e e@@ i @@@@ i@@ uu y-uu ou ei aa oi y y-u@@ o)
       (f _epsilon_ f v )
       (g _epsilon_ g jh zh th f ng k t)
       (h _epsilon_ h @@ )
       (i _epsilon_ i@@ i @@ ii ai @@@@ y ai-@@ aa a)
       (j _epsilon_ h zh jh i y )
       (k _epsilon_ k ch )
       (l _epsilon_ l @@-l l-l)
       (m _epsilon_ m @@-m n)
       (n _epsilon_ n ng n-y )
       (o _epsilon_ @@ ou o oo uu u au oi i @@@@ e uh w u@@ w-uh y-@@)
       (p _epsilon_ f p v )
       (q _epsilon_ k )
       (r _epsilon_ r @@@@ @@-r)
       (s _epsilon_ z s sh zh )
       (t _epsilon_ t th sh dh ch d )
       (u _epsilon_ uu @@ w @@@@ u uh y-uu u@@ y-u@@ y-u i y-uh y-@@ e)
       (v _epsilon_ v f )
       (w _epsilon_ w uu v f u)
       (x _epsilon_ k-s g-z sh z k-sh z g-zh )
       (y _epsilon_ i ii i@@ ai uh y @@ ai-@@)
       (z _epsilon_ z t-s s zh )
       (# #)
       ))
@end lisp
Note this is an exhaustive list and (deliberately) says nothing
about the contexts or frequency that these letter to phone pairs appear.
That information will be generated automatically from the training
set.

Once the number of failed matches is signficantly low enough 
let @code{cummulate-pairs} run to completion.  This counts the number
of times each letter/phone pair occurs in allowable alignments.

Next call
@lisp
festival> (save-table "oald-")
@end lisp
with the name of your lexicon.  This changes the cummulation
table into probabilities and saves it.

Restart festival loading this new table
@lisp
festival allowables.scm oald-pl-table.scm
@end lisp
Now each word can be aligned to an equally-lengthed string of phones,
epsilon and multiphones.  
@lisp
festival> (aligndata "oald.train" "oald.train.align")
@end lisp
Do this also for you test set.

This will produce entries like
@lisp
aaronson _epsilon_ aa r ah n s ah n
abandon ah b ae n d ah n
abate ah b ey t _epsilon_
abbe ae b _epsilon_ iy
@end lisp

The next stage is to build features suitable for @file{wagon} to
build models.  This is done by
@lisp
festival> (build-feat-file "oald.train.align" "oald.train.feats")
@end lisp
Again the same for the test set.

Now you
need to constructrure a description file for @file{wagon} for
the given data.  The can be done using the script @file{make_wgn_desc}
provided with the speech tools

Here is an example script for building the models, you will need
to modify it for your particualr database but it shows the basic
processes
@example
for i in a b c d e f g h i j k l m n o p q r s t u v w x y z 
do
   # Stop value for wagon
   STOP=2
   echo letter $i STOP $STOP
   # Find training set for letter $i
   cat oald.train.feats |
    awk '@{if ($6 == "'$i'") print $0@}' >ltsdataTRAIN.$i.feats
   # split training set to get heldout data for stepwise testing
   traintest ltsdataTRAIN.$i.feats
   # Extract test data for letter $i
   cat oald.test.feats |
    awk '@{if ($6 == "'$i'") print $0@}' >ltsdataTEST.$i.feats
   # run wagon to predict model
   wagon -data ltsdataTRAIN.$i.feats.train -test ltsdataTRAIN.$i.feats.test \
          -stepwise -desc ltsOALD.desc -stop $STOP -output lts.$i.tree
   # Test the resulting tree against
   wagon_test -heap 2000000 -data ltsdataTEST.$i.feats -desc ltsOALD.desc \
              -tree lts.$i.tree
done
@end example
The script @file{traintest} splits the given file @file{X} into @file{X.train}
and @file{X.test} with every tenth line in @file{X.test} and the rest
in @file{X.train}.  

This script can take a significnat amount of time to run, about 6 hours
on a Sun Ultra 140.

Once the models are created the must be collected together into
a single list structure.  The trees generated by @file{wagon}
contain fully probability distributions at each leaf, at this time
this information can be removed as only the most probable will
actually be predicted.  This substantially reduces the size of the
tress.
@lisp
(merge_models 'oald_lts_rules "oald_lts_rules.scm")
@end lisp
(@code{merge_models} is defined within @file{lts_build.scm})
The given file will contain a @code{set!} for the given variable
name to an assoc list of letter to trained tree.  Note the above
function naively assumes that the letters in the alphabet are
the 26 lower case letters of the English alphabet, you will need
to edit this adding accented letters if required.  Note that
adding "'" (single quote) as a letter is a little tricky in scheme
but can be done---the command @code{(intern "'")} will give you 
the symbol for single quote.

To test a set of lts models load the saved model and call 
the following function with the test align file
@lisp
festival oald-table.scm oald_lts_rules.scm
festival> (lts_testset "oald.test.align" oald_lts_rules)
@end lisp
The result (after showing all the failed ones), will be a table showing
the results for each letter, for all letters and for complete words.
The failed entries may give some notion of how good or bad the result
is, sometimes it will be simple vowel diferences, long versus short,
schwa versus full vowel, other times it may be who consonants missing.
Remember the ultimate quality of the letter sound rules is how adequate
they are at providing @emph{acceptable} pronunciations rather than 
how good the numeric score is.

@cindex stress assignment
@cindex predicting stress
For some languages (e.g. English) it is necessary to also find a
stree pattern for unknown words.  Ultimately for this to work well
you need to know the morphological decomposition of the word.
At present we provide a CART trained system to predict stress
patterns for English.  If does get 94.6% correct for an unseen test
set but that isn't really very good.  Later tests suggest that
predicting stressed and unstressed phones directly is actually
better for getting whole words correct even though the models 
do slightly worse on a per phone basis @cite{black98b}.

@cindex compressing the lexicon
@cindex reducing the lexicon
@cindex lexicon compression
As the lexicon may be a large part of the system we have also
experimented with removing entries from the lexicon if the letter to
sound rules system (and stree assignment system) can correct predict
them.  For OALD this allows us to half the size of the lexicon, it could
possibly allow more if a certain amount of fuzzy acceptance was allowed
(e.g. with schwa).  For other languages the gain here can be very
signifcant, for German and French we can reduce the lexicon by over 90%.
The function @code{reduce_lexicon} in @file{festival/lib/lts_build.scm}
was used to do this.  A diccussion of using the above technique as a
dictionary compression method is discussed in @cite{pagel98}.  A
morphological decomposition algorithm, like that described in
@cite{black91}, may even help more.

The technique described in this section and its relative merits with
respect to a number of languages/lexicons and tasks is dicussed more
fully in @cite{black98b}.

@section Post-lexical rules

@cindex post-lexical rules
In fluent speech word boundaries are often degraded in a way that causes
co-articulation accross boundaries.  A lexical entriy should normally
provide pronuncations as if the word is being spoken in isolation.  It
is only once the word has been inserted into the the context in which
it is going to spoken can co-articulary effects be applied.  

Post lexical rules are a general set of rules which can modify the
segment relation (or any other part of the utterance for that matter),
after the basic pronunciations have been found.  In Festival
post-lexical rules are defined as functions which will be applied
to the utterance after intonational accents have been assigned.

@cindex r deletion
For example in British English word final /r/ is only produced when the
following word starts with a vowel.  Thus all other word final /r/s need
to be deleted.  A Scheme function that implements this is as follows
@lisp
(define (plr_rp_final_r utt)
  (mapcar
   (lambda (s)
    (if (and (string-equal "r" (item.name s))  ;; this is an r
             ;; it is syllable final
             (string-equal "1" (item.feat s "syl_final"))
             ;; the syllable is word final
             (not (string-equal "0" 
                   (item.feat s "R:SylStructure.parent.syl_break")))
             ;; The next segment is not a vowel
             (string-equal "-" (item.feat s "n.ph_vc")))
        (item.delete s)))
   (utt.relation.items utt 'Segment)))
@end lisp
@cindex possesive
In English we also use post-lexical rules for phenomena
such as vowel reduction and schwa deletion in the possessive @samp{'s}.

         
        






