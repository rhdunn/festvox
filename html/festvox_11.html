<HTML>
<HEAD>
<!-- This HTML file has been created by texi2html 1.52
     from ../doc/festvox.texi on 2 August 2000 -->

<TITLE>Building Voices in the Festival Speech Synthesis System - 11  Limited domain synthesis</TITLE>
</HEAD>
<BODY bgcolor="#ffffff">
Go to the <A HREF="festvox_1.html">first</A>, <A HREF="festvox_10.html">previous</A>, <A HREF="festvox_12.html">next</A>, <A HREF="festvox_21.html">last</A> section, <A HREF="festvox_toc.html">table of contents</A>.
<P><HR><P>


<H1><A NAME="SEC63" HREF="festvox_toc.html#TOC63">11  Limited domain synthesis</A></H1>

<P>
<A NAME="IDX178"></A>
<A NAME="IDX179"></A>
This chapter discusses and gives examples of building synthesis systems
for limited domains.  By limited domain, we mean applications where the
speech output is constrained.  Such domains may still be infinite but
they may be target to specific vocabulary and phrases.  In fact with
today's current speech system such limited domain applications are in
fact the most common.  Some typical examples are telling the time,
reading telephone numbers.  However from experience we can see that this
technique can be extended to include more general information giving
systems and dialog systems, such as reading the weather, or even the
DARPA Communicator domain (flight information dialog system).

</P>
<P>
Limited domains are discussed here as it is felt that it should be easier
to build unit selection type synthesizers for domains where there are a
much smaller and controlled number of units.  The second reason is that
general TTS systems (e.g. diphone systems) still sound synthetic.
General unit selection when its good, offers near human
quality, but when its bad it is usually much worse than a diphone
synthesizer.  Hybrid systems look interesting but as we cannot yet
automatically detect when general unit selection systems go bad, its not
clear when a diphone system should be swapped in.  But as unit selection
offers so much promise, it is hoped that in a limited domain we can get
the unit selection good quality, and avoid the bad quality.  Finally,
although full TTS systems may be our ultimate goal actually for many
existing systems a limited domain synthesizer is adequate.

</P>
<P>
There is a stage beyond limited domain, but falling short of general
one synthesis, where the most common phrases are the best synthesized
and the quality gracefully degrades as the phrases become less common.
Some hybrid recorded prompts/unit selection/diphone systems have been
proposed and should be able to deliver and answer but we will not deal
directly with those here.

</P>
<P>
However one point you quickly find is that although most speech dialog
systems are very constrained in their vocabulary many require the
hardest class of words: proper names.

</P>
<P>
In continuing in mode of tutorial this chapter first gives a complete
walkthrough of a talking clock.  This is a small example which will
probably work.  Following through this example will give you a good idea
of what is involved in building a limited domain synthesizer.  Also
in the following section problems and modifications can be better 
discussed with respect to this complete example.

</P>


<H2><A NAME="SEC64" HREF="festvox_toc.html#TOC64">11.1  designing the prompts</A></H2>

<P>
To get a good limited domain synthesizer, it is important to understand
what is going on so you can properly tailor your application to take
proper advantage of what can be good, and to avoid the limitations these
methods impose.

</P>
<P>
Note that you may wish to change your application to take better
advantage of this by making its output forms more regular, or at least
use a smaller vocabulary.  As a first basic approximation the techniques
here require that the training contain all words which are to be
actually synthesized.  Therefore if a word does not appear in the
training data it <EM>cannot</EM> be synthesized.  We do provide fall back
positions, using a diphone voice, but that will always be worse than the
more natural unit selection synthesis.  Often this isn't much of a
restriction, or you can tailor your application to avoid having a large
vocabulary.  For example if you are going to build a system for reading
weather reports you can make the weather reports not actually name the
city/town they refer to and just use phrases like "This city ..." and
depend of context for the user to know which actually city is being
talked about.  

</P>
<P>
Of course many speech applications have limited vocabularies in all but
a very few places.  Proper names such as places, people, movie names,
etc are in general complete open classes.  Building a speech application
around those aspects isn't easy and may make a limit domain synthesizer
just impractical.  But it should be noted that those open classes are
also the classes that more general synthesizers will often fail on too.
Some hybrid system may better solve that, which we will not really deal
with here.

</P>
<P>
For almost closed class, recording and modify the data may be a solution
but we have not yet got enough experience to comment on this yet but we
feel that may be a reasonable compromise.  

</P>
<P>
The most difficult part of building a limited domain synthesizer is
designed the database to record that best covers what you wish the
synthesizer to say.  Sometimes this is fairly easy in that you
wish the synthesizer to simple read utterances in a very standard
form where slot will be filled with varying values (such as, dates
numbers etc.)  Such as

<PRE>
The area code you require is NUMBER NUMBER NUMBER.
</PRE>

<P>
The prompts can be devised to fill in values for each of the NUMBER
variables.  

</P>
<P>
More complex utterance can still be viewed in this way

<PRE>
The weather at TIME on DATE: outlook OUTLOOK, NUMBER degrees.
</PRE>

<P>
But once we move into more general dialog its appears initially harder to
properly find the utterances that cover the domain.

</P>
<P>
The first important observation to make is that in such systems where
limited domain synthesis is practical the phrases to be spoken are
almost certainly generated by a computer.  That is there exists an
explicit function which the language generated by the applications.  In
some case this will take the form of an explicit grammar.  In this case
we can use that grammar to generate phrase language and then select from
them utterance that adequately cover the domain.  However even when
there is an explicit grammar it usually will not allow explicitly encode
the frequency of each generated utterance.  As we wish to ensure that
the most common phrases are synthesized best we ideally need to know
which utterances are to be synthesized most often to properly select
which utterance to record.

</P>
<P>
Where a system is already running with a standard synthesizer it is
possible to record what is currently being said and how often.  We can
then use such logs of current system usage to select which utterance
should be in our set of prompts to record.

</P>
<P>
In practice you will be design the system's output at the same time as
the limited domain synthesizer so some combination, and guessing of the
frequency and cover will be necessary.

</P>
<P>
In general you should design your databases to have at least 2 (and
probably 5) examples of each word in you vocabulary.  Secondly you
should select utterances that maximise bi-gram coverage.  That is try to
ensure as many different word-word pairings over your corpus.  We have
used techniques based on these recommendations to greedily select
utterances from larger corpora to record.

</P>


<H2><A NAME="SEC65" HREF="festvox_toc.html#TOC65">11.2  customizing the synthesizer front end</A></H2>

<P>
Once you decided on a set of utterances that appropriately cover the
domain you also need to consider how those particular text strings are
synthesized.  For example if the data contains flight numbers, dates,
times etc, you must ensure that festival properly renders those.  As we
are discussing a limited domain the distribution of token types will be
different from standard text but also more constrained so simple changes
to the lexicon, token to word rules, etc. will allow properly synthesis
of these utterances.

</P>
<P>
One particular area of customization we have noted is worthwhile is that
of phrasing.  It seems important to explicitly mark phrasing in the
prompts, and have the speaker follow such phrasing as it allows for much
better joins in unit selection, as well as consist prosody from the
speaker.  Thus in the default code provided below the normal phrasing
module in festival is replaced with one that treat punctuation as phrasal
markers.

</P>


<H2><A NAME="SEC66" HREF="festvox_toc.html#TOC66">11.3  autolabelling issues</A></H2>

<P>
We currently use autolabel the recorded prompts using the alignment
technique based on <CITE>malfrere97</CITE> that we discussed above for diphone
labelling.  Although this technique works its is not as robust for
general speech as it is for carefully articulated nonsense words.
Specifically this technique does not allow for alternative pronunciations
which are common.

</P>
<P>
Ideally we should use a more generally speech recognition systems.  In
fact for labelling of unit selection database the bets results would be
to train a recognition using Baum-Welch on the database until
convergence.  This would give the most consistent labelling.  We have
started initial experiments with using the open source CMU Sphinx
recognition system are are likely to provide scripts to do this in later
releases.

</P>
<P>
In the mean time the greatest problem in predict phone list must be the
same (or very similar) to what was actually spoken.  This can be
achieved by a knowledge speaker, and by customizing the front end of the
synthesizer so it produces more natural segments.

</P>
<P>
Two observations are worth mentioning here.  First if the synthesizer
makes a mistake in pronunciation and the human speaker does not we
have found the things may work out anyway.  For example we found
the word "hwy" appeared in the prompts, which the synthesizer
pronounced as "H W Y" while the speaker said "highway".  The forced
alignment however cause the phones in the pronunciation of the letters
"H W Y" to be aligned with the phones in "highway" thus on selection 
appropriate, though misnamed, phones were selected.  However you should
<EM>depend</EM> on such compound errors.

</P>
<P>
The second observation is that festival includes prediction of vowel
reduction.  We are beginning to feel that such prediction is unnecessary
in limited domain synthesizer or in unit selection in general.  This
vowel variation can itself be achieve but the clustering technique
themselves and hence allows a reasonable back-off selection strategy
than making firm type decisions.

</P>


<H2><A NAME="SEC67" HREF="festvox_toc.html#TOC67">11.4  unit size and type</A></H2>

<P>
The basic cluster unit selection code available in festival uses
segments as the size of unit.  However the acoustic distance measure
used in cluster uses significant portions of the previous segment.  Thus
the cluster unit selection effectively selects diphones from the
database.  

</P>
<P>
The type of units the cluster selection code uses is based on the
segment name, by default.  In the case of limited domain synthesis we
have found that constraining this further gives both better, and faster
synthesis.  Thus we allow for the unit type to be defined by an
arbitrary feature.  In the default limited domain set up
we use 

<PRE>
SEGMENT_WORD
</PRE>

<P>
That is, the segment plus the word the segment comes from.  Note
this doesn't mean we are doing <EM>word</EM> concatenation in our
synthesizer.  We are still selecting phone units but that the
these phone are differentiated depending on the word they come from
thus a /t/ from the word "unit" cannot be used to synthesis a /t/ in 
"table".  The primary reason for us doing this was to cut down
the search, though it notable improves synthesis quality to.  As we
have constructed the database to have good coverage this is a practical
thing to do.

</P>
<P>
The feature function <CODE>clunit_name</CODE> constructs the unit type for a
particular segment item.  We have provided the above default (segment
name plus (downcased) word name), but it is easy to extend this.

</P>
<P>
In one domain we have worked in we wish to differentiate between words
in different prosody contexts.  Particularly we wished to mark words us
"questionable" so we can ask users for confirmation.  To do this we
marked the "questionable" words in the prompts with a question mark
prefix.  We then recorded them with appropriate intonation and then
defined our <CODE>clunit_name</CODE> function in include "C_" is the word was
prefixed by a question mark.  For example the following two prompts
will be read in a different manner

<PRE>
theater is Squirrel Hill Theater
theater is ?Squirrel ?Hill ?Theater
</PRE>

<P>
Likewise in unit selection the units in the word "Squirrel" will not be
used to synthesize the word "?Squirrel" and vice versa.  Although crude,
this does give simple control over prosody variation though this technique
can require the vocabulary of the units to increase to where this
technique ceases to be practical.

</P>
<P>
It would be good if this technique had a back-off strategy where if no
unit can be found for a particular word it would allow other words to
contribute candidates.  This is ultimately what general unit selection
is.  We do consider this our goal in unit type but in the interest of
building quick and reliable limited domain synthesizers we do not yet do
this but consider it an area we will experiment with.  One specific area
that only partially cross this line is in the synthesis of numbers.  It
seem very reasonable to allow selection of units from simple numbers
(e.g. "seven" and "seventy") but we have not experimented on that yet.

</P>
<P>
One further important point should be highlights about this method
for defining unit types.  Although including the word name in the unit
name does greatly encourage whole words to be selected it does not
mean that joins in the synthesize utterances only occur at word 
boundaries.  It is common that contiguous units are selection from
different occurrences of the same word.  Mid-word (e.g. within
vowels, or at stops) joins at stable places are common.  The optimal
coupling technique selects the best place within a word for the cross
over between two different parts of the database.

</P>


<H2><A NAME="SEC68" HREF="festvox_toc.html#TOC68">11.5  using limited domain synthesizers</A></H2>

<P>
The goal of building such limited domain synthesizer is not just
to show off good synthesis.  We followed this route as we see this
as a very practical method for building speech output systems.

</P>
<P>
For practical reasons, the default configures includes the possibility of
a back-up voice that will be called to do synthesis if the limited
domain synthesizer fails, which for this default setup means the phrase
includes a out of vocabulary word.  It would perhaps be more useful if
the fall position just required synthesis of the out of vocabulary word
itself rather than the whole phrase, but that isn't as trivial as it
might be.  The limit domain synthesis does not prosody modification of
the selections, except for pitch smooth at joins, thus slotting in a
diphone one word would sound very bad.  At present each limited domain
synthesizer has an explicitly defined <CODE>closest_voice</CODE>.  This voice
is used when the limited domain synthesis fails and also when generating
the prompts, which can be looked upon as absolute minimal case when the
synthesizer has no data to synthesize from.

</P>
<P>
There are also issues in speed here, which we are still trying to
improve.  This technique should in fact be fast but it is still slower
than our diphone synthesizer.  One significant reason is the cost if
finding the optimal join put in selected units.  Also this synthesizer
technique require more memory that diphones as the cepstrum parameters
for the whole database are required at run time, in addition to the full
waveforms.  These issues we feel can and should be addressed as these
techniques are not fundamentally computationally expensive so we intend
to work on these aspect in later releases.

</P>


<H2><A NAME="SEC69" HREF="festvox_toc.html#TOC69">11.6  Telling the time</A></H2>

<P>
<A NAME="IDX180"></A>
Festival includes a very simple little script that speaks the current
time (<TT>`festival/examples/saytime'</TT>).  This section explains how to
replace the synthesizer used from this script with one that talks with
your own voice.  This is an extreme example of a limited domain
synthesizer but it is a good example as it allows us to give
a walkthrough of the stages involved in building a limited domain
synthesizer.  This example is also small enough that it can
be done in well under an hour.

</P>
<P>
Following through this example will give a reasonable understanding
of the relative importance of many important steps in the voice
building process.

</P>
<P>
The following tasks are required:

<UL>
<LI>Designing the prompts

<LI>Customized the synthesizer front end

<LI>Recording the prompts

<LI>Autolabelling the prompts

<LI>Building utterance structures for recorded utterances

<LI>Extracting pitchmark and building LPC coefficients

<LI>Building a clunit based synthesizer from the utterances

<LI>Testing and tuning

</UL>

<P>
Before starting set the environment variables <TT>`FESTVOXDIR'</TT> and
<TT>`ESTDIR'</TT> to the directories which contain the festvox distribution
and the Edinburgh Speech Tools respectively.  Under bash and other good
shells this may be done by commands like

<PRE>
export FESTVOXDIR=/home/awb/projects/festvox
export ESTDIR=/home/awb/projects/1.4.1/speech_tools
</PRE>

<P>
In earlier releases we only offered a command line based method for
building voices and limited domain synthesizers.  In order to make the
process easier and less prone to error we have introduced and graphical
front end to these scripts.  This front end is called
<CODE>pointyclicky</CODE> (as it offers a pointy-clicky interface).  It is
particularly useful in the actual prompting and recording.  Although
<CODE>pointyclicky</CODE> is the recommend route in the section we go through the
process step by step to give a better understanding of what is required
and where problems may lie that require attention.

</P>
<P>
A simple script is provided setting up the basic directory structure
and copying in some default parameter files.  The festvox distribution
includes all the setup for the time domain.  When building
for your domain, you will need to provide the file <TT>`etc/DOMAIN.data'</TT>
contains your prompts (as described below).

<PRE>
mkdir ~/data/time
cd ~/data/time
$FESTVOXDIR/src/ldom/setup_ldom cmu time awb
</PRE>

<P>
As in the definition of diphone databases we require three identifiers
for the voice.  These are (loosely) institution, domain and speaker.
Use <TT>`net'</TT> if you feel there isn't an appropriate institution for
you, though we have also use the project name that the voice is being
build for here.  The domain name seems well defined.  For speaker name
we have also used style as opposed to speaker name.  The primary reason
for these to so that people do not all build limited domain synthesizer
with the same thus making it not possible to load them into the same
instance of festival.

</P>
<P>
This setup script makes the directories and copies basic scheme 
files into the <TT>`festvox/'</TT> directory.  You may need to
edit these files later.

</P>


<H3><A NAME="SEC70" HREF="festvox_toc.html#TOC70">11.6.1  Designing the prompts</A></H3>

<P>
In this <CODE>saytime</CODE> example the basic format of the utterance is

<PRE>
The time is now, &#60;exactness&#62; &#60;minute info&#62; &#60;hour info&#62;, in the &#60;day info&#62;.
</PRE>

<P>
For example

<PRE>
The time is now, a little after five to ten, in the morning.
</PRE>

<P>
In all there are 1152 (4x12x12x2) utterances (although there are
three possible day info parts (morning, afternoon and evening) they
only get 12 hours, 6 hours and 6 hours respectively).  Although it
would technically be possible to record all of these we wish to reduce
the amount of recording to a minimum.  Thus what we actually do is 
ensure there is at least one example of each value in each slot.

</P>
<P>
Here is a list of 24 utterances that should cover the main
variations.

<PRE>
The time is now, exactly five past one, in the morning
The time is now, just after ten past two, in the morning
The time is now, a little after quarter past three, in the morning
The time is now, almost twenty past four, in the morning
The time is now, exactly twenty-five past five, in the morning
The time is now, just after half past six, in the morning
The time is now, a little after twenty-five to seven, in the morning
The time is now, almost twenty to eight, in the morning
The time is now, exactly quarter to nine, in the morning
The time is now, just after ten to ten, in the morning
The time is now, a little after five to eleven, in the morning
The time is now, almost twelve.
The time is now, just after five to one, in the afternoon
The time is now, a little after ten to two, in the afternoon
The time is now, exactly quarter to three, in the afternoon
The time is now, almost twenty to four, in the afternoon
The time is now, just after twenty-five to five, in the afternoon
The time is now, a little after half past six, in the evening
The time is now, exactly twenty-five past seven, in the evening
The time is now, almost twenty past eight, in the evening
The time is now, just after quarter past nine, in the evening
The time is now, almost ten past ten, in the evening
The time is now, exactly five past eleven, in the evening
The time is now, a little after quarter to midnight.
</PRE>

<P>
These examples are first put in the prompt file with an utterance
number and the prompt in double quotes like this.

<PRE>
(time0001 "The time is now ...")
(time0002 "The time is now ...")
(time0003 "The time is now ...")
...
</PRE>

<P>
These prompt should be put into <TT>`etc/DOMAIN.data'</TT>.  This file is
used by many of the following sub-processes.

</P>


<H3><A NAME="SEC71" HREF="festvox_toc.html#TOC71">11.6.2  Recording the prompts</A></H3>

<P>
<A NAME="IDX181"></A>
The best way to record the prompts is to use a professional speaker in a
professional recording studio (anechoic chamber) using dual channel (one
for audio and the other for the electroglottograph signal) direct to
digital media using a high quality head mounted microphone.

</P>
<P>
However most of us don't have such equipment (or voice talent) so
readily available so whatever you do will probably have to be a
compromise.  The head mounted mike requirement is the cheapest to meet
and it is pretty important so you should at least meet that requirement.
Anechoic chambers are expensive, and even professional recording studios
aren't easy to access (though most Universities will have some such
facilities).  It is possible to do away with the EGG reading if
a little care is taken to ensure pitchmarks are properly extracted 
from the waveform signal alone.

</P>
<P>
We have been successful in recording with a standard PC using a standard
soundblaster type 16bit audio card though results do vary from machine
to machine.  Before attempting this you should record a few examples on
the PC to see how much noise is being picked up by the mike.  For example
try the following

<PRE>
$ESTDIR/bin/na_record -f 16000 -time 5 -o test.wav -otype riff
</PRE>

<P>
<A NAME="IDX182"></A>
<A NAME="IDX183"></A>
<A NAME="IDX184"></A>
This will record 5 seconds from the microphone in the machine you run
the command on.  You should also do this to test that the microphone
is plugged in (and switched on).  Play back the recorded wave with
<TT>`na_play'</TT> and perhaps play with the mixer levels until you get
the least background noise with the strongest spoken signal.  Now
you should display the waveform to see (as well as hear) how much
noise is there.

<PRE>
$FESTVOXDIR/src/general/display_sg test.wav
</PRE>

<P>
This will display the waveform and its spectrogram.  Noise will show up
in the silence (and other) parts.

</P>
<P>
<A NAME="IDX185"></A>
There a few ways to reduce noise.  Ensure the microphone cable isn't
wrapped around other cables (especially power cables).  Turning the
computer 90 degrees may help and repositioning things can help too.
Moving the sound board to some other slot in the machine can also help
as well as getting a different microphone (even the same make).

</P>
<P>
There is a large advantage in recording straight to disk as it allows
the recording to go directly into right files.  Doing off-line recording
(onto DAT) is better in reducing noise but transferring it to disk and
segmenting it is a long and tedious process.

</P>
<P>
Once you have checked your recording environment you can proceed with
the build process.

</P>
<P>
First generate the prompts with the command

<PRE>
festival -b festvox/build_ldom.scm '(build_prompts "etc/time.data")'
</PRE>

<P>
and prompt and record them with the command

<PRE>
bin/prompt_them etc/time.data
</PRE>

<P>
You may or may not find listening to the prompts before speaking
useful.  Simply displaying them may be adequate for you (if so 
comment out the <TT>`na_play'</TT> line in <TT>`bin/prompt_them'</TT>).

</P>


<H3><A NAME="SEC72" HREF="festvox_toc.html#TOC72">11.6.3  Autolabelling the prompts</A></H3>

<P>
The recorded prompt can be labelled by aligning them against
the synthesize prompts.  This is done by the command

<PRE>
bin/make_labs prompt-wav/*.wav
</PRE>

<P>
If the utterances are long (&#62; 10 seconds of speech) you may require lots
of swap space to do this stage (this could be fixed).

</P>
<P>
<A NAME="IDX186"></A>
<A NAME="IDX187"></A>
Once labelled you should check that they are labelled
reasonable.  The labeller typically gets it pretty much correct,
or very wrong, so a quick check can often save time later.  You 
can check the database using the command

<PRE>
emulabel etc/emu_lab
</PRE>

<P>
Once you are happy with the labelling you can construct the
whole utterance structure for the spoken utterances.  This is done
by combining the basic structure from the synthesized prompts and
the actual times from the automatically labelled ones.  This
can be done with the command

<PRE>
festival -b festvox/build_ldom.scm '(build_utts "etc/time.data")'
</PRE>



<H3><A NAME="SEC73" HREF="festvox_toc.html#TOC73">11.6.4  Extracting pitchmarks and building LPC coefficients</A></H3>

<P>
<A NAME="IDX188"></A>
Getting good pitchmarks is important to the quality of the synthesis,
see section <A HREF="festvox_15.html#SEC114">15.4  Extracting pitchmarks from waveforms</A> for more detailed
discussion on this issue.  For the limited domain synthesizers the
pitch extract is a little less crucial that for diphone collection.
Though spending a little time on this does help.

</P>
<P>
If you have recorded EGG signals the you can use <TT>`bin/make_pm'</TT>
from the <TT>`.lar'</TT> files.  Note that you may need to add (or remove)
the option <TT>`-inv'</TT> depending on the updownness of your EGG signal.
However so far only the CSTR larygnograph seems to produce inverted
signals so the default should be adequate.  Also note the
parameters that specify the pitch period range, <TT>`-min'</TT> and <TT>`max'</TT>
the default setting are suitable for a male speaker, for a female you
should modify these to something like

<PRE>
-min 0.0033 -max 0.0875 -def 0.005
</PRE>

<P>
The changing from a range of (male) 200Hz-80Hz with a default of
100Hz, to a female range of 300Hz-120Hz and default of 200Hz.

</P>
<P>
<A NAME="IDX189"></A>
If you don't have an EGG signal you must extract the pitch from the
waveform itself.  This works though may require a little modification of
parameters, and it is computationally more expensive (and wont be as
exact as from an EGG signal).  There are two methods, one
using Entropic's <TT>`epoch'</TT> program which work pretty well
without tuning parameters.  The second is to use the free Speech
Tools program <TT>`pitchmark'</TT>.  The first is very computationally
expensive, and as Entropic is no longer in existence, the program
is no longer available (though rumours circulate that it may
appear again for free).  To use <TT>`epoch'</TT> use the program

<PRE>
bin/make_pm_epoch wav/*.wav
</PRE>

<P>
To use <TT>`pitchmark'</TT> use the command

<PRE>
bin/make_pm_wave wav/*.wav
</PRE>

<P>
As with the EGG extraction <TT>`pitchmark'</TT> uses parameters to specify
the range of the pitch periods, you should modify the parameters to best
match your speakers range.  The other filter parameters also 
can make a different to the success.   Rather than try to explain
what changing the figures mean (I admit I don't fully know), the
best solution is to explain what you need to obtain as a result.

</P>
<P>
Irrespective of how you extract the pitchmarks we have found that
a post-processing stage that moves the pitchmarks to the nearest
peak is worthwhile.  You can achieve this by

<PRE>
bin/make_pm_fix pm/*.pm
</PRE>

<P>
<A NAME="IDX190"></A>
<A NAME="IDX191"></A>
<A NAME="IDX192"></A>
At this point you may find that your waveform file is upside down.
Normally this wouldn't matter but due to the basic signal processing
techniques we used to find the pitch periods upside down signals confuse
things.  People tell me that it shouldn't happen but some recording
devices return an inverted signal.  From the cases we've seen the same
device always returns the same form so if one of your recordings is
upside down all of them probably are (though there are some published
speech databases e.g. BU Radio data, where a random half are upside
down).  

</P>
<P>
In general the higher peaks should be positive rather than negative.
If not you can invert the signals with the command

<PRE>
for i in wav/*.wav
do
   ch_wave -scale -1.0 $i -o $i
done
</PRE>

<P>
If they are upside, invert them and re-run the pitch marking.  (If
you do invert them it is not necessary to re-run the segment labelling.)

</P>
<P>
Power normalization can help too.  This can be done globally by the
function

<PRE>
bin/simple_powernormalize wav/*.wav
</PRE>

<P>
This should be sufficient for full sentence examples.  In the diphone
collection we take greater care in power normalization but that vowel
based technique will be too confused by the longer more varied examples.

</P>
<P>
<A NAME="IDX193"></A>
<A NAME="IDX194"></A>
Once you have pitchmarks, next you need to generate the pitch
synchronous MELCEP parameterization of the speech used in building the
cluster synthesizer.

<PRE>
bin/make_mcep wav/*.wav
</PRE>



<H3><A NAME="SEC74" HREF="festvox_toc.html#TOC74">11.6.5  Building a clunit based synthesizer from the utterances</A></H3>

<P>
Building a full clunit synthesizer is probably a little bit of over kill
but the technique basically works.  See section <A HREF="festvox_10.html#SEC53">10  Unit selection databases</A>
for a more detailed discussion of this technique.  The basic
parameter file <TT>`festvox/time_build.scm'</TT>, is reasonable as
a start.

<PRE>
festival -b festvox/build_ldom.scm '(build_clunits "etc/time.data")'
</PRE>

<P>
If all goes well this should create a file
<TT>`festival/clunits/cmu_time_awb.catalogue'</TT> and set of index trees in
<TT>`festival/trees/cmu_time_awb_time.tree'</TT>.

</P>


<H3><A NAME="SEC75" HREF="festvox_toc.html#TOC75">11.6.6  Testing and tuning</A></H3>

<P>
To test the new voice start Festival as

<PRE>
festival festvox/cmu_time_awb_ldom.scm '(voice_cmu_time_awb_ldom)'
</PRE>

<P>
The function <TT>`(saytime)'</TT> can now be called and it should
say the current time, or <CODE>(saythistime "11:23")</CODE>.

</P>
<P>
Note this synthesizer can <EM>only</EM> say the phrases that it has phones
for which basically means it can only say the time in the format given
at the start of this chapter.  Thus although you can
use <TT>`SayText'</TT> you can only five it text in the write form
if you expect it to works.  That's what limited domain synthesis
is.

</P>
<P>
A full directory structure of this example with the recordings and
parameters files is available at <A HREF="http://www.festvox.org/examples/cmu_time_awb_ldom/">http://www.festvox.org/examples/cmu_time_awb_ldom/</A>.  And an on-line demo of this voice in that directory is
available at <A HREF="http://www.festvox.org/ldom/index.html">http://www.festvox.org/ldom/index.html</A>.

</P>


<H2><A NAME="SEC76" HREF="festvox_toc.html#TOC76">11.7  Making it better</A></H2>

<P>
The above walkthough is to give you a basic idea of the stages involved in
building a limited domain synthesizer.  The quality of a limited domain
synthesizer will most likely be excellent in parts and very bad in
others which is typical of techniques like this.  Each stage is, of
course, more complex than this and there are a number of things that can
be done to improve it.

</P>
<P>
For limited domain synthesize it should be possible to correct 
the errors such that it is excellent always.  To do so though requires
being able to diagnose where the problems are.  The most
likely problems are listed here

<UL>
<LI>Mis-labelling

Due to lipsmacks, and other reasons the labelling may not be correct.
The result may the wrong, extra or missing segments in the synthesized
utterance.  Using <TT>`emulabel'</TT> you can check and hand correct the
labels.
<LI>Mis-spoken data

The speaker may have made a mistake in the content.  This can often
happen even when the speaker is careful.  Mistakes can be actual
content (it is easy to read a list of number wrongly), but also
hesitations and false starts can make the recording bad.  Also note
that inconsistent prosodic variation can also affect the synthesis
quality.   Re-recording can be considered for bad examples,
or you can delete them from the <TT>`etc/LDOM.data'</TT> list, assuming
there is enough variation in the rest of the examples to ensure
proper coverage of the domain.
<LI>Bad pitchmarking

Automatic pitchmarking is not really automatic.  It is very worthwhile
checking to see if it is correct and re-running the pitchmarking with
better parameters until it is better.  (We need better documentation
here on how to know what "correct" is.)
<LI>Looking at the data

There is never a substitute for actually looking at the data.  Use
<TT>`emulabel'</TT> to actually look at the recorded utterances and see what
the labelling is.  Ensure these match and files haven't got out of
order.  Look at a random selection not just the first example.
<LI>Improving the unit clustering

The clustering techniques and the features used here are pretty generic
and by no means optimal.  Even for the simple example given here it is
not very good.  See the chapter on section <A HREF="festvox_10.html#SEC53">10  Unit selection databases</A> for
more discussion on this.  Adding new features for use in cluster may
help a lot.
</UL>

<P>
The line between limited domain synthesis and unit selection is fuzzy.
The more complex and varied the phrases you synthesize are, the more
difficult it is to produce reliable synthesis.

</P>

<P><HR><P>
Go to the <A HREF="festvox_1.html">first</A>, <A HREF="festvox_10.html">previous</A>, <A HREF="festvox_12.html">next</A>, <A HREF="festvox_21.html">last</A> section, <A HREF="festvox_toc.html">table of contents</A>.
</BODY>
</HTML>
