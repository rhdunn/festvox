<HTML>
<HEAD>
<!-- This HTML file has been created by texi2html 1.52
     from ../doc/festvox.texi on 24 Febuary 2000 -->

<TITLE>Building Voices in the Festival Speech Synthesis System - 4  Utterances</TITLE>
</HEAD>
<BODY bgcolor="#ffffff">
Go to the <A HREF="festvox_1.html">first</A>, <A HREF="festvox_3.html">previous</A>, <A HREF="festvox_5.html">next</A>, <A HREF="festvox_17.html">last</A> section, <A HREF="festvox_toc.html">table of contents</A>.
<P><HR><P>


<H1><A NAME="SEC14" HREF="festvox_toc.html#TOC14">4  Utterances</A></H1>



<H2><A NAME="SEC15" HREF="festvox_toc.html#TOC15">4.1  Utterance structure</A></H2>

<P>
<A NAME="IDX38"></A>
<A NAME="IDX39"></A>
<A NAME="IDX40"></A>
The basic building block for Festival is the <EM>utterance</EM>.  The
structure consists of a set of <EM>relations</EM> over a set of
<EM>items</EM>.  Each item represents a object such as a word, segment,
syllable, etc. while relations relate these items together.  An item may
appear in multiple relations, such as a segment will be in a
<CODE>Segment</CODE> relation and also in the <CODE>SylStructure</CODE> relation.
Relations define an ordered structure over the items within them, in
general these may be arbitrary graphs but in practice so far we have
only used <EM>lists</EM> and <EM>trees</EM> Items may contain a number of
features.

</P>
<P>
There are no built-in relations in Festival and the names and use of
them is controlled by the particular modules used to do synthesis.
Language, voice and module specific relations can easy be created and
manipulated.  However within our basic voices we have followed a number
of conventions that should be followed if you wish to use some of the
existing modules.

</P>
<P>
<A NAME="IDX41"></A>
The relation names used will depend on the particular structure chosen
for your voice.  So far most of our released voices have the same
basic structure though some of our research voices contain quite 
a different set of relations.  For our basic English voices the
relations used are as follows
<DL COMPACT>

<DT><CODE>Text</CODE>
<DD>
<A NAME="IDX42"></A>
Contains a single item which contains a feature with the input character
string that is being synthesized
<DT><CODE>Token</CODE>
<DD>
<A NAME="IDX43"></A>
A list of trees where each root of each tree is the white space
separated tokenized object from the input character string.  Punctuation
and whitespace has been stripped and placed on features on these token
items.  The daughters of each of these roots are the list of words
that the token is associated with.   In many cases this is a one
to one relationship, but in general it is one to zero or
more.  For example tokens comprising of digits will typically
be associated with a number of words.
<DT><CODE>Word</CODE>
<DD>
<A NAME="IDX44"></A>
<A NAME="IDX45"></A>
The words in the utterance.  By <EM>word</EM> we typically mean something
that can be given a pronunciation from a lexicon (or letter to sound
rules).  However in most of our voices we distinguish pronunciation by
the words and a part of speech feature.  Words with also be leaves of the
<CODE>Token</CODE> relation, leaves of the <CODE>Phrase</CODE> relation and roots of
the <CODE>SylStructure</CODE> relation.
<DT><CODE>Phrase</CODE>
<DD>
<A NAME="IDX46"></A>
A simple list of trees representing the prosodic phrasing on the
utterance.  In our voices we only have one level of prosodic phrase
below the utterance (though you can easily add a deeper hierarchy
if your models require it).  The tree roots are labelled with
the phrase type and the leaves of these trees are in the
<CODE>Word</CODE> relation.
<DT><CODE>Syllable</CODE>
<DD>
<A NAME="IDX47"></A>
A simple list of syllable items.  These syllable items are intermediate
nodes in the <CODE>SylStructure</CODE> relation allowing access to the words
these syllables are in and the segments that are in these syllables.
In this format no further onset/coda distinction is made explicit but can
be derived from this information.
<DT><CODE>Segment</CODE>
<DD>
<A NAME="IDX48"></A>
A simple list of segment (phone) items.  These form the leaves of
the <CODE>SylStructure</CODE> relation through which we can find where each
segment is placed within its syllable and word.  By convention
silence phones do not appear in any syllable (or word) but will 
exist in the segment relation.
<DT><CODE>SylStructure</CODE>
<DD>
<A NAME="IDX49"></A>
A list of tree structures over the items in the <CODE>Word</CODE>, 
<CODE>Syllable</CODE> and <CODE>Segment</CODE> items.
<DT><CODE>IntEvent</CODE>
<DD>
<A NAME="IDX50"></A>
A simple list of intonation events (accents and boundaries).
These are related to syllables through the <CODE>Intonation</CODE> relation.
<DT><CODE>Intonation</CODE>
<DD>
<A NAME="IDX51"></A>
A list of trees whose roots are items in the <CODE>Syllable</CODE> relation,
and daughters are in the <CODE>IntEvent</CODE> relation. It is assumed that a
syllable may have a number of intonation events associated with it (at
least accents and boundaries), but an intonation event may only by
associated with one syllable.
<DT><CODE>Wave</CODE>
<DD>
<A NAME="IDX52"></A>
A relation consisting of a single item that has a feature with the
synthesized waveform.
<DT><CODE>Target</CODE>
<DD>
<A NAME="IDX53"></A>
A list of trees whose roots are segments and daughters are
F0 target points.  This is only used by some intonation modules.
<DT><CODE>Unit, SourceSegments, Frames, SourceCoef TargetCoef</CODE>
<DD>
A number of relations used the the <CODE>UniSyn</CODE> module.
</DL>



<H2><A NAME="SEC16" HREF="festvox_toc.html#TOC16">4.2  Modules</A></H2>

<P>
<A NAME="IDX54"></A>
The basic synthesis process in Festival is viewed as applying
a set of <EM>modules</EM> to an utterance.  Each module will access
various relations and items and potentially generate new features,
items and relations.  Thus as the modules are applied the utterance
structure is filled in with more and more relations until
ultimately the waveform is generated.

</P>
<P>
<A NAME="IDX55"></A>
Modules may be written in C++ or Scheme.  Which modules are executed are
defined in terms of the utterance <CODE>type</CODE>, a simple feature on the
utterance itself.  For most text to speech cases this is defined to be
of type <CODE>Tokens</CODE>.  The function <CODE>utt.synth</CODE> simply looks up an
utterance's type and then looks up the definition of the defined
synthesis process for that type and applies the named modules.
Synthesis types maybe defined using the function <CODE>defUttType</CODE>.
For example definition for utterances of type <CODE>Tokens</CODE>
is

<PRE>
(defUttType Tokens
  (Token_POS utt) 
  (Token utt)        
  (POS utt)
  (Phrasify utt)
  (Word utt)
  (Pauses utt)
  (Intonation utt)
  (PostLex utt)
  (Duration utt)
  (Int_Targets utt)
  (Wave_Synth utt)
  )
</PRE>

<P>
While a simpler case is when the input is phone names
and we don't wish to do all that text analysis and prosody 
prediction. Then we use the type <CODE>Phones</CODE> which simply
loads the phones, applies fixed prosody and the synthesizes
the waveform

<PRE>
(defUttType Phones
  (Initialize utt)
  (Fixed_Prosody utt)
  (Wave_Synth utt)
  )
</PRE>

<P>
In general the modules named in the type definitions are general and
actually allow further selection of more specific modules within
them.  For example the <CODE>Duration</CODE> module respects the global
parameter <CODE>Duration_Method</CODE> and will call then desired duration
module depending on this value.  

</P>
<P>
When building a new voice you will probably not need to change any of
these definitions, though you may wish to add a new module and we will
show how to do that without requiring any change to the synthesis
definitions in a later chapter.

</P>
<P>
There are many modules in the system, some simply wraparounds
to choose between other modules.  However the basic modules
used for text to speech have the basic following function
<DL COMPACT>

<DT><CODE>Token_POS</CODE>
<DD>
<A NAME="IDX56"></A>
basic token identification, used for homograph disambiguation
<DT><CODE>Token</CODE>
<DD>
<A NAME="IDX57"></A>
Apply the token to word rules building the <CODE>Word</CODE> relation.
<DT><CODE>POS</CODE>
<DD>
<A NAME="IDX58"></A>
<A NAME="IDX59"></A>
A standard part of speech tagger (if desired)
<DT><CODE>Phrasify</CODE>
<DD>
<A NAME="IDX60"></A>
Build the <CODE>Phrase</CODE> relation using the specified method.  Various
are offered, from statistically trained models to simple CART trees.
<DT><CODE>Word</CODE>
<DD>
<A NAME="IDX61"></A>
Lexical look up building the <CODE>Syllable</CODE> and <CODE>Segment</CODE>
relations and the <CODE>SylStructure</CODE> related these together.
<DT><CODE>Pauses</CODE>
<DD>
<A NAME="IDX62"></A>
Prediction of pauses, inserting silence into the <CODE>Segment</CODE>
relation, again through a choice of different prediction mechanisms.
<DT><CODE>Intonation</CODE>
<DD>
<A NAME="IDX63"></A>
Prediction of accents and boundaries, building the <CODE>IntEvent</CODE>
relation and the <CODE>Intonation</CODE> relation that links IntEvents
to syllables.  This can easily be parameterized for most practical
intonation theories.
<DT><CODE>PostLex</CODE>
<DD>
<A NAME="IDX64"></A>
Post lexicon rules that can modify segments based on their
context.  This is used for things like vowel reduction,
contractions, etc.
<DT><CODE>Duration</CODE>
<DD>
<A NAME="IDX65"></A>
Prediction of durations of segments.
<DT><CODE>Int_Targets</CODE>
<DD>
<A NAME="IDX66"></A>
The second part of intonation.  This creates the <CODE>Target</CODE>
relation representing the desired F0 contour.
<DT><CODE>Wave_Synth</CODE>
<DD>
<A NAME="IDX67"></A>
A rather general function that in turn calls the appropriate
method to actually generate the waveform.
</DL>



<H2><A NAME="SEC17" HREF="festvox_toc.html#TOC17">4.3  Utterance access</A></H2>

<P>
<A NAME="IDX68"></A>
A set of simple access methods exist for utterances, relations,
items and features, both in Scheme and C++.  As much as possible these
access methods are as similar as possible.

</P>
<P>
As the users of this document will primarily be accessing utterance via
Scheme we will describe the basic Scheme functions available for access
and give some examples of idioms to achieve various standard functions.

</P>
<P>
In general the required arguments to a lisp function are reflected in
the first parts of the name of the function.  Thus
<CODE>item.relation.next</CODE> requires an item, and relation name and will
return the next item in that named relation from the given one.

</P>
<P>
A listing a short description of the major utterance access and
manipulation functions is given in the Festival manual.

</P>
<P>
An important notion to be aware of is that an item is always viewed
through so particular relation.  For example, assuming
a typically utterance called <CODE>utt1</CODE>.

<PRE>
(set! seg1 (utt.relation.first utt1 'Segment))
</PRE>

<P>
<CODE>seg1</CODE> is an item viewed from the <CODE>Segment</CODE> relation.  Calling
<CODE>item.next</CODE> on this will return the next item in the <CODE>Segment</CODE>
relation.  A <CODE>Segment</CODE> item may also be in the <CODE>SylStructure</CODE>
item. If we traverse it using next in that relation we will hit
the end when we come to the end of the segments in that syllable.

</P>
<P>
<A NAME="IDX69"></A>
You may <EM>view</EM> a given item from a specified relation by
requesting a view from that.  In Scheme <CODE>nil</CODE> will
be returned if the item is not in the relation.  The
function <CODE>item.relation</CODE> takes an item and relation
name and returns the item as view from that relation.

</P>
<P>
Here is a short example to help illustrate the basic
structure.

<PRE>
(set! utt1 (utt.synth (Utterance Text "A short example.")))
</PRE>

<P>
The first segment in <CODE>utt!</CODE> will be silence. 

<PRE>
(set! seg1 (utt.relation.first utt1 'Segment))
</PRE>

<P>
This item will be a silence as can shown by

<PRE>
(item.name seg1)
</PRE>

<P>
If we find the next item we will get the schwa representing the 
indefinite article.

<PRE>
(set! seg2 (item.next seg1))
(item.name seg2)
</PRE>

<P>
Let us move onto the "sh" to illustrate the different between
traversing the <CODE>Segment</CODE> relation as opposed to the 
<CODE>SylStructure</CODE>

<PRE>
(set! seg3 (item.next seg2))
</PRE>

<P>
Let use define a function which will take an item, print its
name name call next on it <EM>in the same relation</EM> and 
continue until it reaches the end.

<PRE>
(define (toend item) 
  (if item
      (begin
       (print (item.name item))
       (toend (item.next item)))))
</PRE>

<P>
If we call this function on <CODE>seg3</CODE> which is in the <CODE>Segment</CODE>
relation we will get a list of all segments until the end of the utterance

<PRE>
festival&#62; (toend seg3)
"sh"
"oo"
"t"
"i"
"g"
"z"
"aa"
"m"
"p"
"@"
"l"
"#"
nil
festival&#62;
</PRE>

<P>
However if we first changed the view of seg3 to the <CODE>SylStructure</CODE>
relation we will be traversing the leaf nodes of the syllable structure
tree which will terminate at the end of that syllable.

<PRE>
festival&#62; (toend (item.relation seg3 'SylStructure)
"sh"
"oo"
"t"
nil
festival&#62; 
</PRE>

<P>
Note that <CODE>item.next</CODE> returns the item immediately to the next in
that relation.  Thus it return <CODE>nil</CODE> when the end of a sub-tree is
found.  <CODE>item.next</CODE> is most often used for traversing simple lists
through it is defined for any of the structure supported by relations.
The function <CODE>item.next_item</CODE> allows traversal of any relation
returning a next item until it has visiting them all.  In the simple
list case this this equivalent to <CODE>item.next</CODE> but in the tree case
it will traverse the tree in <EM>pre-order</EM> that is it will visit
roots before their daughters, and before their next siblings.

</P>
<P>
<A NAME="IDX70"></A>
<A NAME="IDX71"></A>
Scheme is particularly adept at using functions as first class
objects.  A typical traversal idiom is to apply so
function to each item in a a relation.  For example support
we have a function<CODE>PredictDuration</CODE> which takes a single item 
and assigns a duration.  We can apply this to each item in the
<CODE>Segment</CODE> relation

<PRE>
(mapcar
 PredictDuration
 (utt.relation.items utt1 'Segment))
</PRE>

<P>
The function <CODE>utt.relation.items</CODE> returns all items in the
relation as a simple lisp list.  

</P>
<P>
Another method to traverse the items in a relation is use
the <CODE>while</CODE> looping paradigm which many people are more
familiar with.

<PRE>
(let ((f (utt.relation.first utt1 'Segment)))
  (while f
   (PredictDuration f)
   (set! f (item.next_item f))))
</PRE>

<P>
If you wish to traverse only the leaves of a tree you
may call <CODE>utt.relation.leafs</CODE> instead of 
<CODE>utt.relation.items</CODE>.  A leaf is defined to be an item with
no daughters.  Or in the <CODE>while</CODE> case, there isn't standardly
defined a <CODE>item.next_leaf</CODE> but code easily be defined
as 

<PRE>
(define (item.next_leaf i)
  (let ((n (item.next_item i)))
   (cond
    ((null n) nil)
    ((item.daughters n) (item.next_leaf n))
    (t n))))
</PRE>



<H3><A NAME="SEC18" HREF="festvox_toc.html#TOC18">4.3.1  Features as pathnames</A></H3>

<P>
<A NAME="IDX72"></A>
Rather than explicitly calling a set of functions to find your way round
an utterance we also allow access through a linear flat <EM>pathname</EM>
mechanism.  This mechanism is read-only but can succinctly access not
just features on a given item but features on related items too.

</P>
<P>
For example rather than calling an explicit next function
to find the name of the following item thus

<PRE>
(item.name (item.next i))
</PRE>

<P>
You can access it via the pathname

<PRE>
(item.feat i "n.name")
</PRE>

<P>
Festival will interpret the feature name as a pathname.  In addition
to traversing the current relation you can switch between
relations via the element <CODE>R:&#60;relationame&#62;</CODE>.  Thus to
find the stress value of an segment item <CODE>seg</CODE> we need
to switch to the <CODE>SylStructure</CODE> relation, find its parent
and check the <CODE>stress</CODE> feature value.

<PRE>
(item.feat seg "R:SylStructure.parent.stress")
</PRE>

<P>
Feature pathnames make the definition of various prediction
models much easier.  CART trees for example simply specify
a pathname as a feature, dumping features for training is also
a simple task.  Full function access is still useful when 
manipulation of the data is required but as most access is 
simply to find values pathnames are the most efficient way to 
access information in an utterance.

</P>


<H3><A NAME="SEC19" HREF="festvox_toc.html#TOC19">4.3.2  Access idioms</A></H3>

<P>
For example suppose you wish to traverse each segment in an
utterance replace all vowels in unstressed syllables with a
schwa (a rather over-aggressive reduction strategy but it servers
for this illustrative example.

<PRE>
(define (reduce_vowels utt)
 (mapcar
  (lambda (segment)
   (if (and (string-equal "+" (item.feat segment "ph_vc"))
            (string-equal 
             "1" (item.feat segment "R:SylStructure.parent.stress")))
        (item.set_name segment "@")))
  (utt.relation.items 'Segment)))
</PRE>



<H2><A NAME="SEC20" HREF="festvox_toc.html#TOC20">4.4  Utterance building</A></H2>

<P>
<A NAME="IDX73"></A>
<A NAME="IDX74"></A>
<A NAME="IDX75"></A>
As well as using Utterance structures in the actual runtime
process of converting text to speech we also use them in 
database representation.  Basically we wish to build utterance
structures for each utterance in a speech database.  Once they
are in that structure, as if they had been (correctly) synthesized,
we can use these structures for training various models.  For example
given the actually durations for the segments in a speech database
and utterance structures for these we can dump the actual durations
and features (phonetic, prosodic context etc.) which we feel influence
the durations and train models on that data.

</P>
<P>
Obviously real speech isn't as clean as synthesized speech so its not
always easy to build (reasonably) accurate utterances for the real
utterances.  However here we will itemize a number of functions that
will make the building of utterance from real speech easier.  Building
utterance structures is probably worth the effort considering how
easy it is to build various models from them.  Thus we recommend
this even though at first the work may not immediately seem
worthwhile.

</P>
<P>
In order to build an utterance of the type used for our English voices
(and which is suitable for most of the other languages we have done),
you will need label files for the following relations.  Below
we will discuss how to get these labels, automatically, by 
hand or derived from other label files in this list and the relative
merits of such derivations.

</P>
<P>
<A NAME="IDX76"></A>
The basic label types required are
<DL COMPACT>

<DT><CODE>Segment</CODE>
<DD>
segment labels with (near) correct boundaries, in the phone set
of your language.
<DT><CODE>Syllable</CODE>
<DD>
Syllables, with stress marking (if appropriate) whose boundaries
are closely aligned with the segment boundaries.
<DT><CODE>Word</CODE>
<DD>
Words with boundaries aligned (close) to the syllables and segments.
By <EM>words</EM> we mean the things which can be looked up in a lexicon
thus <SAMP>`1986'</SAMP> would not be considered a word and should be
rendered as three words <SAMP>`nineteen eighty six'</SAMP>.
<DT><CODE>IntEvent</CODE>
<DD>
Intonation labels aligned to a syllable (either within the syllable
boundary or explicitly naming the syllable they should align to.  If
using ToBI (or some derivative) these would be standard ToBI labels,
while in something like Tilt these would be <SAMP>`a'</SAMP> and <SAMP>`b'</SAMP>
marking accents and labels.
<DT><CODE>Phrase</CODE>
<DD>
A name and marking for the end of each prosodic phrase.
<DT><CODE>Target</CODE>
<DD>
The mean F0 value in Hertz at the mid-point of each segment
in the utterance.
</DL>

<P>
<A NAME="IDX77"></A>
<A NAME="IDX78"></A>
<A NAME="IDX79"></A>
Segment labels are probably the hardest to generate.  Knowing what
phones are there can only really be done by actually listening to the
examples and labelling them.  Any automatic method will have to make low
level phonetic classifications which machines are not particularly good
at (nor are humans for that matter).  Some discussion of autoaligning
phones is given in the diphone chapter where an aligner distributed with
this document is described.  This may help but as much depends on the
segmental accuracy getting it right ultimately hand correction at least
is required.  We have used that aligner on a speech database though we
already knew from another (not so accurate) aligner what the phone
sequences probably were.  Our aligner improved the quality of exist
labels and the synthesizer (phonebox) that used it, but there are
external conditions that made this a reasonably thing to do.

</P>
<P>
<A NAME="IDX80"></A>
Word labelling can most easily be done by hand, it is much
easier than to do than segment labelling.  In the continuing process
of trying to build automatic labellers for databases we currently
reckon that word labelling could be the last to be done automatically.
Basically because with word labelling, segment, syllable and intonation
labelling becomes a much more constrained task.  However it is
important that word labels properly align with segment labels even
when spectrally there may not be any real boundary between
words in continuous speech. 

</P>
<P>
<A NAME="IDX81"></A>
Syllable labelling can probably best be done automatically given segment
(and word) labelling. The actual algorithm for syllabification may
change but whatever is chosen (or defined from a lexicon) it is
important that that syllabification is consistently used throughout the
rest of the system (e.g. in duration modelling).  Note that automatic
techniques in aligning lexical specifications of syllabification are in
their nature inexact.  There are multiple acceptable ways to say words
and it is relatively important to ensure that the labelling reflects
what is actually there.  That is simply looking up a word in a lexicon
and aligning those phones to the signal is not necessarily correct.
Ultimately this is what we would like to do but so far we have
discovered our unit selection algorithms are nowhere near robust enough
to do this.

</P>
<P>
<A NAME="IDX82"></A>
The Target labelling required here is a single average F0 value for each
segment.  This currently is done fully automatically from the signal.
This is naive and a better representation of F0 could be more
appropriate, it is used only in some of the model building described
below.  Ultimately it would be good if the F0 need not be explicitly
used at all but just use the factors that determine the F0 value, but
this is still a research topic.

</P>
<P>
<A NAME="IDX83"></A>
Phrases could potentially be determined by a combination of F0 power and
silence detection but the relationship is not obvious.  In general we
hand label phrases as part of the intonation labelling process.
Realistically only two levels of phrasing can reliably be labelled, even
though there are probably more.  That is, roughly, sentence internal and
sentence final, what ToBI would label as (2 or 3) and 4.  More exact
labellings would be useful.

</P>
<P>
<A NAME="IDX84"></A>
<A NAME="IDX85"></A>
<A NAME="IDX86"></A>
For intonation events we have more recently been using Tilt accent
labelling.  This is simpler than ToBI and we feel more reliable.  The
hand labelling part marks <CODE>a</CODE> (for accent) and <CODE>b</CODE> for
boundary.  We have also split boundaries into <CODE>rb</CODE> (rising
boundary) and <CODE>fb</CODE> (falling boundary).  We have been experimenting
with autolabelling these and have had some success but that's still a
research issue.  Because there is a well defined and fully automatic
method of going from a/b labelled waveforms to a parameterization of the
F0 contour we've found Tilt the most useful Intonation labelling.  Tilt
is described in <CITE>taylor99</CITE>.

</P>
<P>
ToBI accent/tone labelling <CITE>silverman92</CITE> is useful too but time
consuming to label.  If it exists for the database then its usually
worth using.

</P>
<P>
<A NAME="IDX87"></A>
In the standard Festival distribution there is a festival
script <TT>`festival/examples/make_utts'</TT> which will build
utterance structures from the labels for the six basic relations.

</P>
<P>
This function can most easily be used given the following
directory/file structure in the database directory.  <TT>`festival/relations/'</TT>
should contain a directory for each set of labels named for the 
utterance relation it is to be part of (e.g. <TT>`Segment/'</TT>, 
<TT>`Word/'</TT>, etc.

</P>
<P>
The constructed utterances will be saved in <TT>`festival/utts/'</TT>.

</P>
<P>
An example of the label files is given with this document
in <A HREF="src/db_example/festival/relations/">src/db_example/festival/relations/</A> and the
build utterance in <A HREF="src/db_example/festival/utts/">src/db_example/festival/utts/</A>

</P>


<H2><A NAME="SEC21" HREF="festvox_toc.html#TOC21">4.5  Extracting features from utterances</A></H2>

<P>
<A NAME="IDX88"></A>
Many of the training techniques that are described in the 
following chapters extract basic features (via pathnames) from 
a set of utterances.  This can most easily be done by the
<TT>`festival/examples/dumpfeats'</TT> Festival script.  It takes
a list of feature/pathnames, as a list or from a file and saves 
the values for a given set of items in a single feature file (or
one for each utterance).  Call <TT>`festival/examples/dumpfeats'</TT>
with the argument <CODE>-h</CODE> for more details.

</P>
<P>
For example suppose for all utterances we want the segment
duration, its name, the name of the segment preceding it
and the segment following it.

<PRE>
dumpfeats -feats "(segment_duration name p.name n.name)" \
    -relation Segment -output dur.feats festival/utts/*.utt
</PRE>

<P>
If you wish to save the features in separate files one
for each utterance, if the output filename contains a <SAMP>`%s'</SAMP>
it will be filled in with the utterance fileid.  Thus to dump
all features named in the file <TT>`duration.featnames'</TT> we
would call

<PRE>
dumpfeats -feats duration.featnames -relation Segment \
         -output feats/%s.dur festival/utts/*.utt
</PRE>

<P>
The file <TT>`duration.featnames'</TT> should contain the features/pathnames
one per line (without the opening and closing parenthesis.

</P>
<P>
Other features and other specific code (e.g. selecting a
voice that uses an appropriate phone set), can be included in this
process by naming a scheme file with the <CODE>-eval</CODE> option.

</P>
<P>
The dumped feature files consist of a line for each
item in the named relation containing the requested feature values
white space separated.  For example

<PRE>
0.399028 pau 0 sh 
0.08243 sh pau iy 
0.07458 iy sh hh 
0.048084 hh iy ae 
0.062803 ae hh d 
0.020608 d ae y 
0.082979 y d ax 
0.08208 ax y r 
0.036936 r ax d 
0.036935 d r aa 
0.081057 aa d r 
...
</PRE>

<P><HR><P>
Go to the <A HREF="festvox_1.html">first</A>, <A HREF="festvox_3.html">previous</A>, <A HREF="festvox_5.html">next</A>, <A HREF="festvox_17.html">last</A> section, <A HREF="festvox_toc.html">table of contents</A>.
</BODY>
</HTML>
